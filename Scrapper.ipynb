{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='www.espn.com', port=80): Max retries exceeded with url: /college-sports/football/recruiting/databaseresults/_/page/90/sportid/24/class/2006/sort/school/starsfilter/GT/ratingfilter/GT/statuscommit/Commitments/statusuncommit/Uncommited (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000195338BF160>: Failed to establish a new connection: [WinError 10065] A socket operation was attempted to an unreachable host'))\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'response' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c3a493faa93b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m259\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mlistingurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseurl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparturl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mlistings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetlistings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistingurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# write to CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c3a493faa93b>\u001b[0m in \u001b[0;36mgetlistings\u001b[1;34m(listingurl)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mlistings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'response' referenced before assignment"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example of web scraping using Python and BeautifulSoup. \n",
    "Sraping ESPN College Football data \n",
    "http://www.espn.com/college-sports/football/recruiting/databaseresults/_/sportid/24/class/2006/sort/school/starsfilter/GT/ratingfilter/GT/statuscommit/Commitments/statusuncommit/Uncommited\n",
    "The script will loop through a defined number of pages to extract footballer data. \n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os \n",
    "import os.path\n",
    "import csv \n",
    "import time \n",
    "\n",
    "\n",
    "def writerows(rows, filename):\n",
    "    with open(filename, 'a', encoding='utf-8') as toWrite:\n",
    "        writer = csv.writer(toWrite)\n",
    "        writer.writerows(rows)\n",
    " \n",
    "\n",
    "def getlistings(listingurl):\n",
    "    '''\n",
    "    scrap footballer data from the page and write to CSV\n",
    "    '''\n",
    "\n",
    "    # prepare headers\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'}\n",
    "\n",
    "    # fetching the url, raising error if operation fails\n",
    "    try:\n",
    "        response = requests.get(listingurl, headers=headers)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    listings = []\n",
    "\n",
    "    # loop through the table, get data from the columns\n",
    "    for rows in soup.find_all(\"tr\"):\n",
    "        if (\"oddrow\" in rows[\"class\"]) or (\"evenrow\" in rows[\"class\"]):          \n",
    "                        \n",
    "            name = rows.find(\"div\", class_=\"name\").a.get_text()\n",
    "            hometown = rows.find_all(\"td\")[1].get_text()\n",
    "            school = hometown[hometown.find(\",\")+4:]\n",
    "            city = hometown[:hometown.find(\",\")+4]\n",
    "            position = rows.find_all(\"td\")[2].get_text()\n",
    "            grade = rows.find_all(\"td\")[4].get_text()\n",
    "\n",
    "            # append data to the list\n",
    "            listings.append([name, school, city, position, grade])\n",
    "\n",
    "    return listings\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    Set CSV file name. \n",
    "    Remove if file alreay exists to ensure a fresh start\n",
    "    '''\n",
    "    filename = \"footballers.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    \n",
    "    '''\n",
    "    Url to fetch consists of 3 parts:\n",
    "    baseurl, page number, year, remaining url\n",
    "    '''\n",
    "    baseurl = \"http://www.espn.com/college-sports/football/recruiting/databaseresults/_/page/\" \n",
    "    page = 1\n",
    "    parturl = \"/sportid/24/class/2006/sort/school/starsfilter/GT/ratingfilter/GT/statuscommit/Commitments/statusuncommit/Uncommited\"\n",
    "\n",
    "    # scrap all pages\n",
    "    while page < 259:\n",
    "        listingurl = baseurl + str(page) + parturl\n",
    "        listings = getlistings(listingurl)\n",
    "\n",
    "        # write to CSV        \n",
    "        writerows(listings, filename)\n",
    "\n",
    "        # take a break\n",
    "        time.sleep(3)\n",
    "\n",
    "        page += 1\n",
    "\n",
    "if page > 1:\n",
    "    print(\"Listings fetched successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
